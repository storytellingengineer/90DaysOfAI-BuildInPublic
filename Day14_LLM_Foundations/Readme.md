# Day 16 – Tokenization & Embeddings  
*#90DaysOfAI • LLM Foundations*  

## 🗝️ Key Concepts Covered  
- **Tokenization paradigms** – how text is split into units that models understand  
- **Sub-word vocabularies** – WordPiece, Byte-Pair Encoding, GPT’s byte-level tokens  
- **Context windows & positional encoding** – why order still matters after splitting  
- **Embeddings** – dense vectors that capture a token’s contextual meaning  
- **Dimensionality reduction** – projecting 768-D embeddings into 2-D space for intuition  

## 🌍 Where These Basics Take You  
| Capability | Idea Spark | Real-World Impact |
|------------|-----------|-------------------|
| **Semantic search** | Turn document corpora into embedding indices | Instantly surface relevant contracts, tickets, or research papers |
| **Personalized recommendation** | Represent users & items in the same vector space | Suggest the right article, song, or product in milliseconds |
| **Anomaly & fraud detection** | Spot “out-of-distribution” vectors | Flag suspicious transactions long before rules-based systems |
| **Zero-shot text classification** | Compare prompt embeddings to label prototypes | Label support emails without task-specific training |
| **Conversational agents** | Retrieve grounding docs via similarity search | Deliver precise answers with minimal hallucination |
| **Domain-specific spell-checking** | Identify odd tokens by distance in embedding space | Correct medical terms or brand names missed by generic spell-check |
| **Content clustering** | Group news or reviews by vector proximity | Summarize thousands of articles into coherent themes |
| **Cross-lingual alignment** | Map multilingual embeddings into a shared space | Power truly global search and analytics |
| **Style transfer & rewriting** | Shift embeddings toward target-tone vectors | Instantly rephrase text for brand voice consistency |
| **Adaptive curriculum learning** | Measure learner answers vs. expert embeddings | Recommend the next lesson that best closes the knowledge gap |

> **Takeaway:** Mastering tokenization and embeddings is like learning the alphabet of modern NLP. Everything from chatbots to fraud detection builds on this foundation.  
