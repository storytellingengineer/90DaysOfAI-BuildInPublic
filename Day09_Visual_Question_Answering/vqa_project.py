# -*- coding: utf-8 -*-
"""vqa_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OFNZZIvvvLEjXE3bp4qsXRDT4aV1JQY9
"""

import torch
from transformers import BlipForQuestionAnswering, BlipProcessor
from PIL import Image
import requests
from io import BytesIO

# Loading pre-trained VQA model (BLIP model)
model_name = "Salesforce/blip-vqa-base"
processor = BlipProcessor.from_pretrained(model_name)
model = BlipForQuestionAnswering.from_pretrained(model_name)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# Defining a function to answer a question based on an image
def answer_question(image_url, question):
    # Load and preprocess the image
    response = requests.get(image_url)
    image = Image.open(BytesIO(response.content)).convert('RGB')

    inputs = processor(image, question, return_tensors="pt").to(device)

    # Forward pass
    outputs = model.generate(**inputs)

    # Decode the output
    answer = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer

# Main logic
if __name__ == "__main__":
    print("Welcome to Visual Question Answering! üì∑‚ùì")

    # User provides input
    img_url = input("Enter the image URL: ")
    user_question = input("Enter your question about the image: ")

    print("\nThinking...\n")
    output_answer = answer_question(img_url, user_question)

    print(f"Your Question: {user_question}")
    print(f"Predicted Answer: {output_answer}")





